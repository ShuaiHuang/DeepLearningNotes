\chapter{Deep Generative Models}

\section{Boltzmann Machines}

\textit{Boltzmann机}是一种基于能量的模型,这也就意味着我们需要使用能量函数来定义联合概率分布
\begin{equation}
P(\bm x)=\frac{\exp(-E(\bm x))}{Z}
\end{equation}
其中$Z$是partition函数,$E$是能量函数
\begin{equation}
E(\bm x)=\bm{-x^TUx-b^Tx}
\end{equation}
其中,$\bm U$是模型参数的权重矩阵,$\bm b$是偏置参数的向量.

从上式可以看出,某个单元的概率可能由其它单元的概率线性组合而成,这一个局限性.而引入隐变量可以打破这种局限性.如果引入隐变量,可以实现对离散变量的概率分布进行全估计.

引入了隐变量之后,能量函数定义为
\begin{equation}
E(\bm x)=\bm{-v^TRv-v^TWh-h^TSh-b^Tv-c^Th}
\end{equation}

\paragraph{Boltamann Machine Learning} Boltamann机学习算法通常基于最大似然度,需要对partition函数进行估计.在学习过程中,连接两个单元的权重更新仅仅与这两个单元的统计量(如$P_{model}(\bm v)$,$\hat P_{data}(\bm v)P_{model}(\bm{h|v})$)有关联.也就是说,学习时局部性的,这在生物学上也是可以接受的.

\section{Restricted Boltzmann Machines}

与Boltzmann机一样,RBM也是基于能量的模型
\begin{equation}
P(\mathbf v=\bm v,\mathbf h=\bm h)=\frac{1}{Z}\exp(-E(\bm{v,h}))
\end{equation}
其中能量$E$定义为
\begin{equation}\label{eq:rbm_energy}
E(\bm{v,h})=\bm{-b^Tv-c^Th-v^TWh}
\end{equation}
没有了$\bm v$与$\bm v$,$\bm h$与$\bm h$的交互项;$Z$是partition函数
\begin{equation}
Z=\sum_{\bm v}\sum_{\bm h}\exp\{-E(\bm{v,h})\}
\end{equation}

\subsection{Conditional Distributions}

虽然$P(\bm v)$很难计算,但是RBM的二部图结构特点使得$P(\mathbf{h|v})$与$P(\mathbf{v|h})$可以相对容易地进行计算以及采样.

原书P$658$推理过程可得,
\begin{equation}
P(\bm{h|v})=\prod_{j=1}^{n_h}\sigma\Big((2\bm h-1)\odot(\bm c+\bm W^T\bm v)\Big)_j
\end{equation}
\begin{equation}
P(\bm{v|h})=\prod_{i=1}^{n_v}\sigma\Big((2\bm h-1)\odot(\bm b+\bm W^T\bm h)\Big)_i
\end{equation}

\subsection{Training Restricted Boltzmann Machines}

RBM可以使用第\ref{ch:partition_function}章中任意一种具有难以计算的partition函数的训练方式(如CD,SML,PCD,ratio matching等)进行训练.

\section{Deep Belief Networks}

\textit{深度信念网络}(deep belief network, DBN)是具有多层隐变量的生成式模型.隐变量是二值离散的,可见单元既可以是离散的,也可以是连续的.图的最上两层是无向的,其余各层是有向的,箭头均指向离数据最近的层.因此DBN的图模型是有向图和无向图的混合.特别地,仅由一层隐层的模型是RBM.

深度信念网络的采样方法是,最上两层隐层使用Gibbs采样,后续层上使用ancestral采样.

深度信念网络使用逐层训练的方法进行训练
\begin{enumerate}
    \item 使用constractive divergence或者stochastic maximum likelihood方法最大化$\mathbb E_{\mathbf v\sim p_{data}}\log p(\bm v)$来训练一个RBM
    \item 第二个RBM使用近似地最大化
    \begin{equation}
    \mathbb E_{\mathbf v\sim p_{data}}\mathbb E_{\mathbf h^{(1)}\sim p^{(1)}(\bm h^{(1)}|\bm v)}\log p^{(2)}(\bm h^(1))
    \end{equation}
    其中$p^{(1)}$是第一个RBM所代表的概率分布,$p^{(2)}$是第二个RBM所代表的概率分布.
    \item 重复以上过程,直到深度信念网络达到预期的层数.
\end{enumerate}
在大多数应用中,不对深度信念网络进行整体训练,但可以通过wake-sleep算法进行参数的精细调节.

虽然深度信念网络是生成式模型,但可以用来提升分类模型的效果.我们可以使用深度信念网络的权重定义一个多层感知器网络(MLP)
\begin{equation}\begin{split}
\bm h^{(1)}&=\sigma(b^{(1)}+\bm v^T\bm W^{(1)})\\
\bm h^{(l)}&=\sigma(b_i^{(l)}+\bm h^{(l-1)T}\bm W^{(l)})
\end{split}\end{equation}
但是这样定义的多层感知器网络忽略了许多深度信念网络中的重要交互.

深度信念网络特制在最深层使用无向连接而在其它层使用指向数据的有向连接的模型.需要特别注意与信念网络进行区分,因为信念网络有时特指有向图模型.

\section{Deep Boltzmann Machines}

\textit{深度Boltzmann机}(deep Boltzmann machine, DBM)是一种多隐层全无向图.

DBM是一种基于能量的模型,假设一个Boltzmann有一个可视层$\bm v$,三个隐层$\bm h^{(1)}$,$\bm h^{(2)}$和$\bm h^{(3)}$,那么联合概率为
\begin{equation}
P(\bm{v,h^{(1)},h^{(2)},h^{(3)}})=\frac{1}{Z(\bm\theta)}\exp\Big(-E(\bm{v,h^{(1)},h^{(2)},h^{(3)};\theta})\Big)
\end{equation}
其中能量定义为
\begin{equation}
E(\bm{v,h^{(1)},h^{(2)},h^{(3)};\theta})=\bm{-v^TW^{(1)}h^{(1)}-h^{(1)T}W^{(2)}h^{(2)}-h^{(2)}W^{(3)}h^{(3)}}
\end{equation}
与RBM的能量定义式\ref{eq:rbm_energy}相比,DBM的能量包含了隐层之间的交互.在后续章节我们可以看到,这些交互一方面会影响模型所表现出的行为,另外一方面也会影响模型的推断.另外,DBM奇偶层之间具有条件独立性.这种二部图结构意味着我们可以使用与RBM相同的条件分布去决定DBM的条件分布.同样由于其条件独立性,Gibbs采样可以在$2$轮迭代内完成.高效率的采样对训练中的随机量最大似然优化算法尤为重要.

\subsection{Interesting Properties}

与DBN相比,DBM的后验概率更为简单,但是DBM的后验概率有更为丰富的估计形式.DBN下估计的下限不一定准确.

DBM的一项缺点是其采样相对困难.

\subsection{DBM Mean Field Inference}

DBM的后验概率使用变分推断很容易实现,尤其是平均场近似.

在变分近似推断中,我们使用一些简单的分布族去逼近一个特定的目标分布.

假设$Q(\bm{h^{(1)},h^{(2)}|v})$是$P(\bm{h^{(1)},h^{(2)}|v})$的近似估计,根据平均场假设有
\begin{equation}
Q(\bm{h^{(1)},h^{(2)}|v})=\prod_jQ(h_j^{(1)}|\bm v)\prod_kQ(h_k^{(2)}|\bm v)
\end{equation}
但是其缺点也很明显,每次使用一个新的$\bm v$的数值都要重新进行推断以确定$Q$的分布.

\subsection{DBM Parameter Learning}

DBM学习过程面临两大挑战:partition函数难以计算;后验概率难以计算.由于DBM包含RBM,所以DBM所遇到的困难同样是RBM所面临的困难.

\subsection{Layer-Wise Pretraining}

从一个随机初始值开始,使用随机最大似然方法训练DBM通常会以失败告终.最朴素同时也是最流行的方法是使用贪心逐层预训练方法.贪心逐层训练方法不仅仅是坐标上升法\footnote{一种优化问题求解方法.},它与坐标上升法有不同之处.

DBM中间每一层都会有自顶向下和自底向上的输入,因此需要对权重做相应的调整.

\subsection{Jointly Training Deep Boltzmann Machines}

DBM在训练过程中的训练效果不容易追踪,因为完整的DBM特性难以评估.Boltzmann机顶部的MLP会让概率模型损失许多优势.

解决DBM训练难题有两种途径:\textit{centered deep Boltzmann machine}和\textit{multi-prediction deep Boltzmann machine}.

\paragraph{Centered Deep Boltzmann Machine}这种方法会产生一个不使用贪心逐层训练策略的模型,但是它并不能像MLP那样经过正则化形成一个分类器.

Centered deep Boltzmann machine引入向量$\bm\mu$减去所有的状态
\begin{equation}
E'(\bm{x;U,b})=\bm{-(x-\mu)^TU(x-\mu)-(x-\mu)^Tb}
\end{equation}
其中,$\bm\mu$是一个超参数,在训练开始时就固定下来.

\paragraph{Multi-Prediction Deep Boltzmann Machine}这种方法使用一种替代训练策略,引入逆传播算法,从而避免对梯度的MCMC估计.

MP-DBM将平均场等式视为recurrent网络族用以近似解决所有可能的推断问题.

图推断引入逆传播算法有两项优势
\begin{itemize}
    \item 训练过程和近似推断过程统一;
    \item 逆传播计算损失函数的精确梯度.
\end{itemize}