\chapter{Applications}

\section{Large-Scale Deep Learning}

但个神经元并不能体现出智能的特性,只有大量神经元组合在一起才能体现出智能的特性.

\subsection{Fast CPU Implementations}

精心调试的CPU会有较高的性能,并且有多种策略可供选择,对运行在CPU上的程序进行优化.

\subsection{GPU Implementations}

绝大部分现代神经网络是在GPU上实现的.GPU具有高度并行性和较大内存带宽,但是相比于CPU,它的时钟频率较低,并且分支能力较弱.人工神经网络有同样的特性,因此适合运行在GPU上.基于此,GPU厂商开发出了\textit{通用GPU}(general purpose GPUs)用以处理GPU编程代码,而不仅仅是图形渲染.

但是GPU编程比较困难,大部分工程人员使用现成的代码进行组合以实现自己的模型,尽量避免编写底层代码.

\subsection{Large-Scale Distributed Implementations}

由于单个机器的计算能力有限,因此我们希望可以将训练过程和推断过程转化成分布式计算.

推断过程的分布式计算很简单,因为每个机器可以分别运行模型的不同部分,处理同一个样本,这又被称为\textit{数据并行}(data parallelism).

多个机器在单个数据点上进行协同计算的过程被称为\textit{模型并行}(model parallelism).

\textit{异步SGD}(asynchronous stochastic gradient descent)可以解决数据并行问题.

\subsection{Model Compression}

模型推断时对内存资源的要求更高.通常使用\textit{模型压缩}(model compression)来获取一个对内存需求更少,运行时间更短的小模型来进行存储和执行.

模型压缩可行的前提是原始模型为防止过拟合将不同的模型进行集成,组成了大模型.

\subsection{Dynamic Structure}

对数据处理系统进行加速的一项通用策略是设计带有在图上具有\textit{动态结构}(dynamic structure)的模型去处理输入数据.在神经网络中根据输入使用不同的子网络进行计算的结构被称为\textit{条件计算}(conditional computation).动态结构是从计算机理论到软件工程领域被广泛使用的一项基本原则.

一种动态结构是层级结构分类器,用以提升计算效率.决策树本身就是典型的动态结构.基于同样的思路,\textit{gater}根据当前输入选择不同的专家网络进行计算.当专家网络数目较少时,使用\textit{hard mixture of experts}进行计算.但是当专家网络数目较多时,组合数目也增多,就需要借助额外的手段去训练专家网络的组合了.

另外一种动态结构是开关形式,隐层神经元根据上下文从不同的输入单元获取输入.

需要注意的是,动态结构大大降低了神经网络的并行化操作,往往很难高效地进行编程实现.

\subsection{Specialized Hardware Implementations of Deep Networks}

神经网络有不同的硬件实现方式:
\begin{itemize}
    \item ASICs: application-specific integrated circuit
    \item digital: based on binary representations of numbers
    \item analog: based on physical implementations of continuous values as voltages or currents
    \item hybird implementations: field programmable gated array
\end{itemize}

深度神经网络专用硬件得以发展的原因有两点:第一,可以在推断时候使用较低的精度;第二,通用处理器(CPU/GPU)计算速率增长放缓,需要通过提升并行特性来提升整体性能.

\section{Computer Vision}

\subsection{Preprocessing}

计算机视觉往往需要很少的预处理步骤.最常见的有像素值归一化和输入图像尺寸归一化.对于训练数据集的预处理有\textit{dataset augmentation}.同时对于训练数据集以及测试数据集进行的预处理主要是为了排除数据扰动对模型的影响.通常需要对于输入数据扰动有明确的描述,并且需要确认去除扰动后对模型训练不会产生影响.