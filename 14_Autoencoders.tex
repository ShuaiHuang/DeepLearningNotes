\chapter{Autoencoders}

Autoencoder是一个试图将输入复制到输出的神经网络.通常只是将输入和输出限制为大致相似,并且只对输入中有用的部分进行描述.传统上autoencoder是降维和特征学习的方式之一,但是近来它也被视为生成式模型的学习方法.尽管autoencoder与前向神经网络有不同之处,它仍旧可以使用\textit{recirculation}\footnote{TODO: 是什么?}进行训练.

\section{Undercomplete Autoencoders}

\textit{Undercomplete autoencoders}的目的在于获取隐变量$\bm h$中有用的特性.因此在模型中对其施加了$\bm h$维度要比$\bm x$维度低的限制.

学习过程可以被视为最小化损失函数
\begin{equation}
L(\bm x,g(f(\bm x)))
\end{equation}
的过程.当$f,g$都是线性函数时,autoencoder可以被视为张开一个和PCA相同的子空间.

但是当encoder和decoder的capacity过高时,autoencoder可能提取不到有用的信息.

\section{Regularized Autoencoders}

在隐变量维度和输入变量维度相同,以及过完备的情况下,autoencoder学不到输入数据的分布.但是借助于正则化,encoder和decoder可以根据模型数据分布训练autoencoder.正则化项可以刺激模型训练出直接复制这种形式以外的表示方法.

除了上述的antoencoder,几乎所有的包含隐变量以及推断过程的生成式模型都可以被视为一种特殊形式的autoencoder.

\subsubsection{Sparse Autoencoders}

\textit{Sparse autoencoders}是指将autoencoder的训练准则中加入稀疏补偿项$\Omega(\bm h)$
\begin{equation}
L(\bm x,g(f(\bm x)))+\Omega(\bm h)
\end{equation}

sparse autoencoders的目的是学习对其他任务有用的特征.但是这种正则化项并没有直观的贝叶斯表示方法.

稀疏补偿项使模型隐变量的分布,它提供了近似训练生成式模型的方法.

\subsection{Denoising Autoencoders}

\textit{Denoising autoencoders}通过改变重构错误项来训练autoencoder,它最小化
\begin{equation}
L(\bm x,g(f(\tilde{\bm x})))
\end{equation}
其中,$\tilde{\bm x}$是被噪声干扰的输入$\bm x$.

\subsection{Regularizing by Penalizing Derivatives}

这种情况下的补偿项为
\begin{equation}
\Omega(\bm{h,x})=\lambda\sum_i\|\nabla_{\bm x}h_i\|^2
\end{equation}
它迫使模型在输入$\bm x$有微小变化时,模型输出变动不会太大.

\section{Representation Power, Layer Size and Depth}

Autoencoder还可以有深层结构.它本身就是一种前向网络,继承了前向网络的所有优势.理论上,包含隐层的深度autoencoder可以近似拟合任何一种函数.

\section{Stochastic Encoders and Decoders}

前向网络的损失函数和输出单元同样可以用于autoencoder.

以激进的观点抛开前向神经网络来看,\textit{encoding function}$f(\bm x)$可以被一般化为\textit{encoding distribution} $p_{encoder}(\bm{h|x})$.

任何一个包含隐变量的模型$p_{encoder}(\bm{h|x})=p_{model}(\bm{h|x})$可以定义一个stochastic encoder
\begin{equation}
p_{encoder}(\bm{h|x})=p_{model}(\bm{h|x})
\end{equation}
和一个stochastic decoder
\begin{equation}
p_{decoder}(\bm{x|h})=p_{model}(\bm{x|h})
\end{equation}

\section{Denoising Autoencoders}

\textit{Denoising autoencoder} (DAE)是一种将受到噪声干扰的数据还原为原始数据的autoencoder.它可以归结为前向神经网络,使用与前向神经网络相同的方法进行训练.

\subsection{Estimating the Score}

\textit{Score mathcing}是极大似然度的替代方法.它是一种对概率分布的一致性估计.它利用的方法是促使模型与数据分布在每个训练数据点上拥有相同的score.DAE一个重要的特性是其训练准则使得autoencoder可以学习出一个向量场($g(f(\bm x))-\bm x$)用以估计数据分布的score.

训练过程中使用Gaussian噪声和均方误差的autoencoder等价于训练一个RBM模型,该模型拥有Gaussian可见单元.

\section{Learning Manifolds with Autoencoders}

和其他的机器学习模型一样,autoencoder模型假设数据集中在低维度的流形或者流行子集上.但是autoencoder不仅学习函数来拟合流形,还需要去学习流行的结构.

流形学习有两个因素
\begin{itemize}
    \item 学习$\bm h$和$\bm x$,建立从输入到输出的映射;
    \item 满足限制条件或者正则化补偿项.
\end{itemize}

流行还有一个重要特性就是其切平面.结合上述两点因素可以看出,流形只对重要变量进行表示,此时只需要对流形切平面上的变化做出反映即可.

最原始的流行学习方法是基于最近邻的非参数化方法.它的缺点是当流行不是很平滑时,需要大量的数据对新数据点进行插值拟合,很难对未知的数据进行泛化.但是常见的AI问题在流形上都是不平滑的,不能通过插值法进行泛化.

\section{Contractive Autoencoders}

\textit{Contractive autoencoder}引入正则项
\begin{equation}
\Omega(\bm h)=\lambda\Big\|\frac{\partial f(\bm x)}{\partial\bm x}\Big\|^2_F
\end{equation}
促使$f$的导数尽可能小.

Contractive autoencoder和denoising autoencoder都能在一定程度上抵抗扰动,但是前者是在特征提取环节实现的,后者是在重构环节实现的.

需要注意的是,contractive autoencoder只是局部收缩,全局来看,可能会存在两个点相互远离的情况.

CAE的目的是为了学习到数据所表示的流行的结构.

深度autoencoder的计算代价较高,并且可能会捕捉到无用的结果.

\section{Predictive Sparse Decomposition}

\textit{Predictive sparse decomposition} (SPD)是稀疏编码和参数autoencoder的结合体.其中,encoder $f(\bm x)$和decoder $h(\bm x)$都是参数化模型.

训练过程可以归结为最小化
\begin{equation}
\|\bm x-g(\bm x)\|^2+\lambda|\bm h|_1+\gamma\|\bm h-f(\bm x)\|^2
\end{equation}

\section{Application of Autoencoders}

Autoencoder被广泛用于降维和信息重获取任务中.

低维度特征可以在许多任务上运用,提升性能.同时,降维也可以提升泛化能力.

在信息重获取任务中,autoencoder可以使得信息检索在某些低维度空间上效率更加高效.具体实现可以参考\textit{语义哈希}(semantic hashing).