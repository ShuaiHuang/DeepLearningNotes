\chapter{Approximate Inference}

概率模型训练的难点在于推断.推断的目的是为了求得\textbf{目标变量的边际分布}或者\textbf{以某些可观测变量为条件的条件分布}.深度学习中推断问题比较困难是由于隐变量间存在交互关系.

\section{Inference as Optimization}

精确推断可以归结为一个优化问题,而近似推断则是精确推断的估计.

假设概率模型包含可观测变量$\mathbf v$和隐变量$\mathbf h$,我们的目的是计算可观测变量的对数概率$\log\,p(\mathbf v;\theta)$.在有些情况下边际化$\mathbf h$很困难,可以计算$\log\,p(\mathbf v;\theta)$的下限$\mathcal L(\mathbf v,\theta,q)$,即\textit{evidence lower bound}(ELBO).ELBO的另一个更常见的名称是\textit{negative variation free energy}.
\begin{equation}
\mathcal L(\mathbf v,\theta,q)=\log\,p(\mathbf v;\theta)-D_{KL}\Big(q(\mathbf{h|v})\big\|p(\mathbf{h|v;\theta})\Big)
\end{equation}
其中$q$是关于$h$的任意分布.

因此我们可以通过最大化$\mathcal L$逼近真实分布进行推断.也可以通过控制$\mathcal L$的松紧程度灵活控制计算量.

\section{Expectation Maximization}

\textit{Expectation maximization}(EM)算法是一种基于最大化$\mathcal L$的推断算法.它适用于训练包含隐变量的模型.但它并不是近似推断的方法,而是学习近似后验概率的方法.

EM算法可以分两步,第一步通过选择分布$q$来最大化$\mathcal L$;第二步通过选择$\theta$来最大化$\mathcal L$\footnote{具体算法步骤可见原书P$634$页下方.}.

EM算法有两点本质:
\begin{enumerate}
    \item 学习过程有一个基本的结构,那就是通过更新模型参数来提升完备数据集的似然度,其中所有的缺失变量可以通过后验概率进行估计填充;
    \item 在EM算法中,我们在更新了$\theta$值之后,仍然可以沿用同一个$q$的值进行训练.
\end{enumerate}

\section{MAP Inference and Sparse Coding}

推断也可以指计算条件概率的过程.另外一种推断是计算出缺失变量的一个最可能的取值,而不是推断出在所有可能取值上的概率分布,即
\begin{equation}
\mathbf h^\ast={\arg\max}_{\mathbf h}p(\mathbf{h|v})
\end{equation}
这又被称为\textit{最大后验概率推断}(maximum a posteriori inference, MAP inference).

通常我们并不将MAP推断视为一种近似推断,因为它并不能准确地计算$\mathbf h^\ast$.如果基于最大化$\mathbf L$的策略进行学习,那么可以视为MAP提出了一个$q$值.从这种程度上来看,MAP推断是一种近似推断,因为它并不能提供最优分布$q$.

MAP推断在深度学习领域既被视为一种特征提取器,又被视为一种学习机制.它主要被用于稀疏编码模型中.

\section{Variational Inference and Learning}

\textit{变分学习}(variational learning)的核心思想是通过限制$q$的分布族来最大化$\mathcal L$.通常$q$分布相对简单或者具有良好的结构.

最常见的$q$的限制是
\begin{equation}
q(\mathbf{h|v})=\prod_iq(h_i|\mathbf v)
\end{equation}
即$q$为乘积分布.这也就是常说的\textit{平均场}(mean field)方法.更一般的,可以使用任何图模型结构来代表$q$,这种通用图模型方法被称为\textit{结构化变分推断}(structured variational inference)\footnote{什么是结构化变分推断?怎样从学习到推断?}

变分方法的一大优势是我们不需要特别指定$q$的参数化形式.即使当隐变量是离散情形时,并不需要变量微积分,但是变量微积分仍然是变分学习或者变分推断的名称来源.

\subsection{Discrete Latent Variables}

隐变量是离散情形的变分推断方法相对直观.基于平均场的方法定义分布$q$,建立查找表,通过查表法确定$q(\mathbf{h|v})$分布.

在确定$q$的行使后就需要优化其参数.要求优化算法的收敛速度要快.\footnote{变分推断具体可见周志华所著的机器学习相关章节,这里略过.}