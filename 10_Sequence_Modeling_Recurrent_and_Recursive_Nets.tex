\chapter{Sequence Modeling: Recurrent and Recursive Nets}

\textit{Recurrent neural networks}(RNN)是专门用来处理序列数据的神经网络.它使用共享参数使得模型可以对不同格式(长度)的数据进行处理.RNN还在一维时域做卷积.

\section{Unfolding Computational Graphs}

计算图中的unfolding操作是将迭代操作转换为重复结构非递归操作的过程.RNN有多种形式,任意一种包含递归的函数均可使用RNN来表示.RNN的典型形式为
\begin{equation}
\mathbf h^{(t)}=f(\mathbf h^{(t-1)}, \mathbf x^{(t)};\theta)
\end{equation}
其中$\mathbf h$是状态变量.由此可见,RNN将$t$时刻任务相关的历史信息经过有损压缩整合成$\mathbf h^{(t)}$.

RNN模型有两种图的表现形式:第一种是使用一个包含所有成分的结点;第二种是将前一种形式的结点进行展开.

结点展开有两个优点:
\begin{enumerate}
    \item 不管序列的长度有多长,学习到的模型始终有相同的输入长度;
    \item 每一步都可以使用相同的转移函数$f$以及相同的参数.
\end{enumerate}
其中,共享参数可以提升模型的泛化能力.

两种形式的计算图各有用处,并无绝对的优劣之分.

\section{Recurrent Neural Networks}

RNN有三种形式
\begin{itemize}
    \item 隐层单元间有recurrent连接,每一步都会输出结果;
    \item 输出层与隐层单元间有recurrent连接,每一步都会输出结果;
    \item 隐层单元间有recurrent连接,但是只有接受了全部的输入后才会输出结果.
\end{itemize}

在RNN反向传播过程中,损失函数的梯度计算代价很高,不能并行化.此外,正向传播时的状态必须一直记录直到反向传播,因此存储代价也很高.因此,需要考虑替代方案.在此,将对unrolled graph上进行并且有$O(\tau)$复杂度的逆向传播算法称为\textit{back-propagation through time}(BPTT).

\subsection{Teacher Forcing and Networks with Output Recurrence}

仅仅在从前一个时间状态的输出单元到后一个时间状态的隐层单元存在连接的RNN网络的学习能力是非常弱的.因为这种类型的网络缺少从隐层到隐层的周期性连接.为了取得较好的效果,就要求输出必须能够捕捉全面的历史信息.由于损失函数中与时间相关的步骤没有耦合,可以对训练过程进行并行化操作.

\textit{Teacher forcing}操作在每一步都是用上一步的正确输出标记作为当前步骤的输入.引入teacher forcing的目的是为了避免BPTT操作.但是,一旦隐层单元所表示的函数是历史时间状态的函数,就必须进行BPTT计算.

\textit{Teacher forcing}的劣势也很明显,在\textit{open-loop}模式下,当网络的输出作为输入的一部分时,输出反馈的输入在训练环节的分布可能和测试环节时的分布有很大差异.对于这个问题,有两种解决方案:
\begin{itemize}
    \item 模型训练时的输入包含teacher forcing输入和free-running输入;
    \item 随机选择使用生成的数值或者实际数据的数值作为输入,以尽量弥补训练时的输入和测试时的输入.
\end{itemize}

\subsection{Computing the Gradient in a Recurrent Neural Network}

RNN中梯度的计算很直接.需要注意的是,由于参数共享,所以共享参数的梯度是将所有时刻的梯度值进行相加求取.

\subsection{Recurrent Networks as Directed Graphical Models}

对于RNN理论上可以使用任何一种损失函数,最常用的是交叉熵损失函数.

一般地,RNN对应的有向图是一个完备图,任何一对输出节点之间都会有有向依赖关系.通常会对图中交互关系较弱的两个节点之间的边进行简化处理.将隐层单元视为随机变量,这样就可以与历史状态进行解耦合.但是,某些操作仍然会有计算代价,比如缺失值填充和参数优化操作.

RNN中的参数共享意味着$t$与$t+1$时刻的条件分布的参数是固定的.

如果从图中进行采样,RNN必须确定序列的长度.具体方法有
\begin{itemize}
    \item 在原始序列中插入特殊分隔符;
    \item 通过伯努利分布判定序列是否结束;
    \item 将长度$\tau$作为预测目标进行学习预测.
\end{itemize}

\subsection{Modeling Sequences Conditioned on Context with RNNs}

RNN对于条件概率分布有着不同的实现方式.当随机变量$\mathbf x$是一个固定长度的向量时,有以下方式将其作为附加信息加入RNN模型用以产生$\mathbf y$序列:
\begin{itemize}
    \item 作为每一个时间步骤的额外输入;
    \item 作为一个初始状态$\mathbf h^{(0)}$;
    \item 以上两种方法的结合.
\end{itemize}
添加的额外信息$\mathbf x$可以视为前向神经网络中的bias.

当有多个输入$\mathbf x^{(t)}$时,可以将$t$时刻的输出作为$t+1$时刻的隐层单元的输入.但是限制条件是,这些序列的长度必须相同.

\section{Bidirectional RNNs}

除了上文提到的当前状态与输入序列中历史值相关的情形,还存在着当前状态与整个输入序列有依赖关系的情形.因此需要引入双向RNN来解决这一问题.在双向RNN中,当前状态对历史状态和未来的状态都存在着依赖关系.

同样地,可以将双向RNN的输入拓展到二维空间,这样就需要在四个方向上分别训练一个RNN.

\section{Encoder-Decoder Sequence-to-Sequence Architectures}

在许多应用情形中,输入序列和输出序列的长度不一定相同.\textit{encoder-decoder}专门用来处理输入输出序列长度不同的情形.其中,encoder用于生成context,decoder用于生成输出序列.context的长度不一定和decoder隐层长度相同.同时encoder-decoder结构还存在一个缺陷,如果context太短的话就不能完整总结较长的输入序列的信息.

\section{Deep Recurrent Networks}

RNN网络中的计算包含3中类型的变换
\begin{itemize}
    \item 从输入到隐状态的变换;
    \item 从前一个隐状态到下一个隐状态的变换;
    \item 从隐状态到输入的变换.
\end{itemize}
RNN中三种操作是一种"浅"变换.为训练出任务所需要的变换,就需要增加深度.

\section{Recursive Neural Networks}

Recursive neural networks与RNN的链式结构不同,它是一种深度树结构.主要被用来处理数据结构.Recursive神经网络中树的深度大大减少,并且可以用来处理长期依赖.数的结构可以独立于数据进行选取.还有外部方法可以用来选择树的结构.除此之外,recursive还有其他的变化类型.