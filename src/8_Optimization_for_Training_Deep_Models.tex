\chapter{Optimization for Training Deep Models}

深度学习过程涉及到的优化过程耗时长且有着重要的地位.本章重点介绍深度神经网络训练过程中的优化算法.一般情况下,训练过程中的优化目标是包含正则化项的损失函数.本章将从以下几个方面介绍优化算法
\begin{enumerate}
    \item 机器学习中的优化问题与传统优化问题的区别;
    \item 深度学习中优化问题面临的挑战;
    \item 自适应学习率或损失函数二阶求导;
    \item 通过简单优化组合出复杂优化的策略.
\end{enumerate}

\section{How Learning Differs from Pure Optimization}

对于机器学习算法中的损失函数$J$,其目的是用来提升机器学习效果$P$的,但是对于传统优化问题来说,$J$仅仅只是一个优化目标.通常情况下,损失函数是在所有训练样本上取平均
\begin{equation}
J(\bm\theta)=\mathbbm E_{(\bm x,y)\sim\hat p_{data}}L(f(\bm x;\bm\theta),y)
\end{equation}
但是我们希望得到的是在数据生成模型分布上的期望泛化误差
\begin{equation}\label{eq:expected_generalization_error}
J^\ast(\bm\theta)=\mathbbm E_{(\bm x,y)\sim p_{data}}L(f(\bm x;\bm\theta),y)
\end{equation}

\subsection{Empirical Risk Minimization}

机器学习的目标是降低期望泛化误差,又被称为\textit{风险}(risk),但是通过观察式\ref{eq:expected_generalization_error}可以看出,真实数据分布$p_{data}$未知,因此期望泛化误差不能通过计算直接得到.因此一个替代方案就是最小化\textit{经验风险}(empirical risk)
\begin{equation}
\mathbbm E_{(\bm x,y)\sim\hat p_{data}}\left[L(f(\bm x;\bm\theta),y)\right]=\frac{1}{m}\sum_{i=1}^mL(f(\bm x^{(i)};\bm\theta),y^{(i)})
\end{equation}
其中$m$是训练样本的数目.

但是直接使用经验风险最小化策略容易产生过拟合,另外由于$0$-$1$损失函数不能直接求导的原因,深度学习中不使用经验风险最小化策略.

\subsection{Surrogate Loss Functions and Early Stopping}

一般从优化问题效率角度出发使用\textit{代理损失函数}(surrogate loss function)对经验风险函数进行替代.代理损失函数可以延长学习的过程,增强训练出的模型的鲁棒性.同时为了避免过拟合,通常使用早停策略,不会将代理损失函数优化到局部极小值点.

\subsection{Batch and Minibatch Algorithms}

机器学习中的优化问题使用部分样本的损失函数加和去估计整体损失函数,进而对参数进行迭代更新.在实际中,从训练集中采样部分样本评估整体风险的做法很常见.使用采样策略对风险进行评估相比于计算准确的风险,会使得算法收敛速度更快.同时,再重复样本上计算梯度会有大量的冗余.

在全部样本上计算梯度的方法被称为\textit{batch/deterministic gradient method},如果使用单个样本计算梯度的方法被称为\textit{stochastic/online method}.深度学习算法中的优化问题介于两者之间,被称为\textit{minibatch stochastic method}.

minibatch尺寸的选择因素有
\begin{itemize}
    \item 从尺寸增大与回报中做平衡;
    \item 从处理器核的数目做考虑;
    \item 从内存角度做考虑;
    \item $2$的倍数;
    \item 小尺寸有正则化的作用.
\end{itemize}

不同的算法会以不同的形式使用minibatch,并使用其中的不同信息.

需要注意的是,minibatch的随机性很重要.实际中常对样本进行随机打乱来保证随机性.如果样本分解到位,可以使用异步并行的方式更新模型参数.

在样本没有重复的条件下,minibatch方法与梯度下降方法的泛化误差同步下降,但是当样本重复(即开始使用样本进行第二轮迭代)后,偏差会上升.在线学习由于会有数据源源不断地输入进来,所以在泛化误差下降的方面更具有优势.在非在线学习的情形下,第一轮是用样本是无偏梯度估计,之后的迭代是为了减小训练误差与测试误差之间的差异.当训练数据集很大时,每个样本只被使用一次,主要关注的是欠拟合和计算效率问题.

\section{Challenges and Neural Network Optimization}

\subsection{Ill-Conditioning}

在优化问题是凸优化的条件下,也会存在挑战.Hessian矩阵病态条件就是其中之一.如式\ref{eq:second_order_taylor_expansion}中的
\begin{equation}
-\epsilon\bm g^T\bm g+\frac{1}{2}\epsilon^2\bm g^T\bm{Hg}
\end{equation}
就有可能存在$\frac{1}{2}\epsilon^2\bm g^T\bm{Hg}>\epsilon\bm g^T\bm g$的病态情况.

其他领域内解决病态的问题的方法不能直接用来解决深度学习中的病态问题,需要做一些调整.

\subsection{Local Minima}

在凸优化问题中,局部极小值等价于全局最小值.匪徒问题局部极小并非全局最小,但这并不是主要问题.

如果一个足够大的训练数据集可以训练出唯一一组模型参数,那么就可以说这个模型是\textit{可辨识模型}(model identifiability).神经网络和其他具有隐变量的模型都是不可辨识的.神经网络具有不可辨识性的原因有
\begin{description}
    \item [weight space symmetry]对隐层调神经元不影响最终结果;
    \item [等价缩放]对某一层的神经元做输入输出的等价缩放,不影响最终结果.
\end{description}

不可辨识性导致局部极小值点很多,但是这些局部极小值点有很多具有等价性.

如果局部极小值点的cost function比全局最小值点的cost function数值高,就会有问题.但是如何定量衡量这种差异还没有一个明确的标准.但是现在,研究者们表示,大多数局部极小值点都会一个比较低的cost function数值.

\subsection{Plateaus, Saddle Points and Other Flat Regions}

\textit{鞍点}(saddle point)可以视为代价函数在某个横截面上的极小值点,同时也是另外一个横截面的极大值点.

对于许多随机函数来说,高维空间中的鞍点数目很多.同时,许多随机函数的Hessian矩阵正特征值越多,越有可能达到代价函数的取值越小.这个性质对于神经网络来说同样适用.

鞍点对基于梯度的优化算法影响并不显著,这些算法可以迅速逃离鞍点位置.相比之下,牛顿法容易陷入鞍点.因此,需要使用二阶优化的saddle-free牛顿法.

牛顿法同样容易陷入极大值点以及平坦区域.

\subsection{Cliffs and Exploding Gradients}

梯度优化算法遇到cliff时,步长会变得很长.为了避免这种\textit{gradient clipping}现象,一个简单的解决方案就是使用固定的步长,除非到了极小值附近的区域.

\subsection{Long-Term Dependencies}

当计算图结构较深时,优化算法就遇到了挑战.如在RNN中,相同的操作被重复多次,就出现了\textit{梯度消失和爆炸问题}(vanishing and exploding gradient problem).但是前向神经网络不存在相同操作被重复多次的结构,因此就不会有梯度消失和爆炸问题.

\subsection{Inexact Gradients}

在实际中,通常只能获取到不精确的Hessian矩阵的估计值.当目标函数很难处理时,梯度也很难被精确计算.因此只能通过使用代理函数来避免上述问题.

\subsection{Poor Correspondence between Local and Global Structure}

当梯度的优化方向不正确,前面所描述的问题即使都被妥善解决,也不能提升优化效果.在深度神经网络的训练过程中,大部分时间都被用来选择合适的步长.在实际中,神经网络并不会恰好达到极大值点,极小值点或者鞍点.

大部分研究关注的是如何选取初始值点,以获的较好的优化结果,而不是在优化算法中使用非局部移动策略.

梯度下降和所有的神经网络训练算法都是基于局部移动策略,如果选取正确的初始值点,就可以得到较好的优化效果.

\subsection{Theoretical Limits of Optimization}

针对神经网络设计的优化算法一般都有效果上限.这些优化算法对神经网络的实际应用影响很小\footnote{TODO: 看一下中文版,搞清楚以这一节是什么意思.}.

\section{Basic Algorithms}

\subsection{Stochastic Gradient Descent}

在\textit{随机梯度下降}(stochastic gradient descent)中,很重要的一个参数就是学习率$\epsilon$.实际使用中,这个参数是动态调整的.

因为SGD通过对样本的采样引入了梯度的噪声,即使损失函数达到了最小值点,噪声也不会消失,为了保证SGD能够收敛,学习率$\epsilon$需要满足
\begin{equation}\begin{split}
\sum_{k=1}^\infty\epsilon_k&=\infty\\
\sum_{k=1}^\infty\epsilon_k^2&<\infty
\end{split}\end{equation}

在迭代过程中,$\epsilon$的调整策略为
\begin{equation}
\epsilon_k=(1-\alpha)\epsilon_0+\alpha\epsilon_\tau
\end{equation}
其中$\alpha=\frac{k}{\tau}$.当迭代轮数达到$\tau$之后,$\epsilon$保持常量.

一般通过代价函数的时间曲线选取合适的$\epsilon$值.

SGD的计算时间不会随着样本数量的增加而增加,同时还可以保证算法的收敛性.为了研究优化算法的收敛性,引入度量指标\textit{excess error}
\begin{equation}
J(\bm\theta)-\min_{\bm\theta}J(\bm\theta)
\end{equation}
对于SGD算法,在第$k$轮迭代后,其excess error为$O(\frac{1}{\sqrt{k}})$,当优化问题是凸优化时,excess error为$O(\frac{1}{k})$.除非引入新的前提假设,这个上限是不会被提升的.但是如果收敛速度超过$O(\frac{1}{k})$时,就会导致过拟合.

\subsection{Momentum}

SGD算法的学习速率较慢,\textit{动量算法}(Momentum Algorithm)是用来加速学习速率的.在动量算法中,引入了速度$\bm v$的概念,用来指示在参数空间中搜索最优参数的速率和方向.为了简便,在这里将动量定义为速度和单位质量质点的乘机.因此有迭代更新公式
\begin{equation}\begin{split}
\bm v&\leftarrow\alpha\bm v-\epsilon\nabla_{\bm\theta}\Big(\frac{1}{m}\sum_{i=1}^mL(\bm f(\bm x^{(i)};\bm\theta),\bm y^{(i)})\Big)\\
\bm\theta&\leftarrow\bm\theta+\bm v
\end{split}\end{equation}
其中,$\alpha\in\left[0,1\right)$用来指示历史梯度均值对当前的影响程度.$\alpha$也可以通过学习算法进行自适应调整.

与物理领域内的动量定义一样,这里的动量也与力$\bm f(t)$和速度$\bm v(t)$有关联.
\begin{equation}\begin{split}
\bm f(t)&=\frac{\partial^2}{\partial t^2}\bm\theta(t)\\
\bm v(t)&=\frac{\partial}{\partial t}\bm\theta(t)\\
\bm f(t)&=\frac{\partial}{\partial t}\bm v(t)
\end{split}\end{equation}
从上式可以看出,动量算法需要解微分方程,可以借助\textit{Euler方法}(Euler's method)\footnote{TODO: 查一下}实现.

动量算法中的力可以视为两个力的合力,一个力正比于损失函数负梯度方向$-\nabla_{\bm\theta} J(\bm\theta)$;另一个力(粘滞阻力)正比于$-\bm v(t)$.当然还有其他形式的阻力,但是最好还是用粘滞阻力的形式.

\subsection{Nesterov Momentum}

\textit{Nesterov Momentum}与标准动量不同之处在于梯度的计算方式.
\begin{equation}\begin{split}
\bm v&\leftarrow\alpha\bm v-\epsilon\nabla_{\bm\theta}\Big(\frac{1}{m}\sum_{i=1}^mL(\bm f(\bm x^{(i)};\bm\theta+\alpha\bm v),\bm y^{(i)})\Big)\\
\bm\theta&\leftarrow\bm\theta+\bm v
\end{split}\end{equation}

Nesterov Momentum将凸优化的exxcess error从$O(\frac{1}{k})$提升至$O(\frac{1}{k^2})$.但是对于非凸问题,则没有提升.

\section{Parameter Initialization Strategies}

有些优化算法直接将目标函数优化到最优点;另外有些优化算法通过迭代的方式将目标函数优化到最优点,并且不需要特别选取优化起始点.深度学习中的优化并没有上述的优势,其优化过程是一个迭代过程,并且优化结果很容易受到起始点的初始值影响.

选取起始点是一个很艰难的任务,因为
\begin{enumerate}
    \item 由于对网络结构没有一个深刻的认识,改进初始化策略很难;
    \item 不知道哪些性质对于深度学习优化是真正有效果的;
    \item 起始点对于泛化能力也有影响,但是并没有一个严格的选取准则.
\end{enumerate}
现在比较明确的一个规则是打破隐层神经元之间的对称性,使得隐层神经元可以学习到更多的上下文信息.显式地去搜索隐层神经元所代表的函数代价太高,因此转而去搜索隐层神经元的权重向量,使其正交.

\textbf{权重初始分布范围对优化过程和模型的泛化能力都会产生影响.}对于优化过程来说,如果出事权重值较大
\begin{itemize}
    \item 有助于打破对称性;
    \item 避免冗余神经元;
    \item 避免前向或者后向传播时信号丢失.
\end{itemize}
但是权重也不能过大,比如在RNN中,权重过大将会使得模型对于输入扰动异常敏感(chaos).但是从泛化能力的角度来看,大权重将会使得迭代过程中权重改变量较小,最终优化结果离起始点更近.一般情况下,带有早停策略的梯度下降法并不等同于权重下降,但是它提供了一种对于初始值选取的简单模拟.

选取权重范围的一个简单策略是从分布$U(-\frac{1}{\sqrt{m}},\frac{1}{\sqrt{m}})$随机采样,其中$m$是网络输入个数,$n$是网络输出的个数.此外还推荐使用\textit{normalized initialization}
\begin{equation}
W_{i,j}\sim U\Big(-\sqrt{\frac{6}{m+m}},\sqrt{\frac{6}{m+m}}\Big)
\end{equation}
上述初始化方法在每层初始激活函数方差相同和所有层初始梯度方差相同之间做了折中.\footnote{TODO: 没明白,看论文怎么说.}

为神经网络中每一层的非线性函数选择合适的\textit{增益因子}(scaling or gain factor $g$),可以保证收敛性与层数独立.在前向神经网络中,激活函数或者梯度的增大或者缩小是一个随机行走行为.如果保持每一层权重的范数,就可以避免梯度消失或者爆炸.但是这些最优的权重初始化准则并不能保证最优的优化效果,原因有
\begin{itemize}
    \item 我们可能使用了错误的准则,即所选取的准则于神经网络的本质并不匹配;
    \item 选取的准则中所使用的特性在优化过程中并没有得到保持;
    \item 选取的准则可能对于优化问题适用,但是对于降低泛化误差并不适用.
\end{itemize}

所有初始权重拥有相同标准差也存在缺陷,当网络层数较多时,每一个权重会比较小.适用\textit{sparse initialization}可以避免这一问题.

在计算资源允许时,将每一层权重的范围设成一个超参数进行搜索,是一个很好的方案.此外,根据单个minibatch数据选择权重范围,也是一个很好地方案,它不会影响泛化误差.

\textbf{除了权重参数外,还需要关注bias的初始化}.通常biase的初始化策略与权重的初始化相匹配.大多数情况下将其初始化为$0$,除此之外,还有
\begin{itemize}
    \item 对于输出层的神经元,将biase初始化为统计量的边缘分布;
    \item 选取bias的初始值时,还需要避免产生saturation;
    \item 有时某个神经元会控制其他神经元是否参与到方程中进行计算,因此将大部分bias设置为$h\approx 1$.
\end{itemize}

\textbf{此外还有方差或者精度的初始化.}一般可以将其设置为$1$.

除了上述方法,还可以使用机器学习的方法初始化参数;使用相关领域内的任务初始化参数(不相关领域的也行).

\section{Algorithms with Adaptive Learning Rates}

\textit{学习率}(learning rate)是一个重要的超参数,因为它对于模型的表现有显著影响.动量算法避免了学习率超参数设置问题,但是它引入了另外一个超参数.如果我们认为不同坐标轴方向上的敏感程度不同,因此就有必要在每个坐标轴方向上设置一个不同的学习率超参数.

\textit{delta-bar-delta算法}(delta-bar-delta algorithm)是一种早期的学习率选取策略,它根据损失函数的偏导符号对学习率进行选取.但它是full batched优化算法.

\subsection{AdaGrad}

AdaGrad对学习率参数做缩放,正比于历史值平方和平方根的倒数.AdaGrad对于某些深度学习模型比较有效,但不是全部模型都有效.

\subsection{RMSProp}

RMSProp算法对AdaGrad算法在非凸问题情形下进行了改进.它在累积平方和中加入了权重.目前RMSProp算法是深度学习算法中一个标准选项.

\subsection{Adam}

Adam算法可以视为RMSProp算法和动量算法的结合.最简单的结合方法是在RMSProp中梯度缩放环节中加入动量算法.Adam还在一阶动量估计和二阶动量初始化中加入了biase修正.

\subsection{Choosing the Right Optimization Algorithm}

选择哪种类型的学习率优化算法并没有共论,这个与使用者的偏好有关.

\section{Approximate Second-Order Methods}

本章的目标函数是经验风险,实际中还有其他形式的目标函数.

\subsection{Newton's Method}

二阶方法使用二阶导数改善优化效果\footnote{TODO: 建立起优化方法的简单知识体系.},最广泛使用的是牛顿法.对$J(\bm\theta)$在$\bm\theta_0$附近进行二阶展开,有
\begin{equation}
J(\bm\theta)\approx J(\bm\theta_0)+(\bm\theta-\bm\theta_0)^T\nabla_{\bm\theta} J(\bm\theta_0)+\frac{1}{2}(\bm\theta-\bm\theta_0)^T\bm H(\bm\theta-\bm\theta_0)
\end{equation}
其中$\bm H$是$J$对$\bm\theta$在$\bm\theta_0$点附近的Hessian矩阵.如果求取critical point,有
\begin{equation}\label{eq:newtown_cirtical_point}
\bm\theta^\ast=\bm\theta_0-\bm H^{-1}\nabla_{\bm\theta} J(\bm\theta_0)
\end{equation}
因此对于二次函数,通过缩放$\bm H^{-1}$可以直接到达最小值点.对于凸优化非二次问题,在Hessian矩阵正定的情况下,需要通过迭代算法获取最优解.

深度学习中的Hessian矩阵并不保证正定,牛顿法的优化方向可能会产生错误,因此需要添加正则化项
\begin{equation}
\bm\theta^\ast=\bm\theta_0-\left[H(f(\bm\theta_0))+\alpha\bm I\right]^{-1}\nabla_{\bm\theta} f(\bm\theta_0)
\end{equation}
正则化后的牛顿法有两个缺陷:
\begin{itemize}
    \item 为保证正定,$\alpha$可能会很大,这样步长就会减小;
    \item 需要计算Hessian矩阵的逆矩阵,计算代价大.
\end{itemize}

\subsection{Conjugate Gradients}

\textit{共轭梯度}(conjugate gradient)通过迭代选取共轭梯度方向来避免对Hessian矩阵求逆.在共轭梯度算法中,我们每一次选取的搜索方向都是与上一次搜索方向共轭的方向.搜索方向$\bm d_t$的迭代形式为
\begin{equation}
\bm d_t=\nabla_{\bm\theta} J(\bm\theta)+\beta_{t-1}\bm d_{t-1}
\end{equation}
$\beta_{t}$的选取方法有
\begin{description}
    \item [Fletcher-Reeves] \begin{equation}\beta_t=\frac{\nabla_{\bm\theta} J(\bm\theta_t)^T\nabla_{\bm\theta} J(\bm\theta_t)}{\nabla_{\bm\theta} J(\bm\theta_{t-1})^T\nabla_{\bm\theta} J(\bm\theta_{t-1})}\end{equation}
    \item [Polak-Ribiere] \begin{equation}\beta_t=\frac{(\nabla_{\bm\theta} J(\bm\theta_t)-\nabla_{\bm\theta} J(\bm\theta_{t-1}))^T\nabla_{\bm\theta} J(\bm\theta_t)}{\nabla_{\bm\theta} J(\bm\theta_{t-1})^T\nabla_{\bm\theta} J(\bm\theta_{t-1})}\end{equation}
\end{description}

共轭梯度法经过修改可用于非线性的情形.\textit{nonlinear conjugate gradients}算法就是应用于非线性情形的算法.

\subsection{BFGS}

\textit{Broyden-Fletcher-Goldfarb-Shanno (BGFS)}算法的目的是为了减少牛顿法中的计算负担.它通过迭代法估计式\ref{eq:newtown_cirtical_point}中的$\bm H^{-1}$,得到估计量$\bm M_t$.并且通过$\bm M_t$决定优化方向$\bm\rho_t=\bm M_t\bm g_t$
\begin{equation}
\bm\theta_{t+1}=\bm\theta_t+\epsilon^\ast\bm\rho_t
\end{equation}

BFGS算法的优势在于通过线性搜索减少线性搜索的时间.缺点在于Hessian逆矩阵的存储,需要$O(n^2)$的存储空间.

\textit{Limited Memory BFGS}或L-BFGS改进了BFGS的存储限制.它添加了一个假设为$\bm M^{(t-1)}$初始值为单位矩阵,并且在迭代过程中仅存储用来更新$\bm M$的向量.

\section{Optimization Strategies and Meta-Algorithms}

\subsection{Batch Normalization}

\textit{批归一化}(batch normalizai)\footnote{TODO: 看相关论文,深化理解.}是一种自适应\textit{再参数化}(reparamterization)方法,主要用来解决深度模型中遇到的困难.

梯度算法有一个前提就是除了当前层,其他所有层的参数都不会改变.但是实际上,所有层的参数都是同步更新的.某一层参数的更新会影响其它所有层的更新.二阶优化将二阶交互考虑进优化算法中,但是计算代价太大.批归一化提供了一种再参数化深度神经网络的方法.

假设$\bm H$是将要被归一化的网络层的最小批激活函数(以设计矩阵形式排列),为了归一化$\bm H$,将其替换为
\begin{equation}
\bm H'=\frac{\bm{H-\mu}}{\bm\sigma}
\end{equation}
其中,$\bm\mu$是神经元的均值向量;$\bm\sigma$是每一个神经元对应的标准差.训练过程中,当进行逆向传播时,进行归一化,避免梯度增大标准差或者均值.在测试时,使用训练得到的参数$\bm\mu$,$\bm\sigma$.

归一化去除了一阶和二阶统计量的影响,但是允许神经元间和非线性统计量的更新.

归一化会降低神经网络的表示能力,因此通常将$\bm H'$替换为$\gamma\bm H'+\beta$.其中$\gamma$和$\beta$通过学习得到.与原始的$\bm H$表示不同,新的表示形势下,均值仅由参数$\beta$决定.

\subsection{Coordinate Descent}

\textit{coordinate descent}和\textit{block coordinate descent}方法分别是指一次优化一个或者一组变量,使目标函数收敛.这两种方法在不同变量相互孤立或者某些变量优化效率较高的情况下才会特别有效.在变量间有相互影响的情况下,并不是一个特别好的解决方案.

\subsection{Polyak Averaging}

\textit{Polyak Averaging}是指对优化路径上的点取平均保证算法的收敛性.非凸问题的优化路径很复杂,Polyak平均的形式为
\begin{equation}
\hat{\bm\theta}^{(t)}=\alpha\hat{\bm\theta}^{(t-1)}+(1-\alpha)\bm\theta^{(t)}
\end{equation}

\subsection{Supervised Pretraining}

\textit{预训练}(pretraining)是指在用以解决复杂任务的模型之前,首先训练简化的模型用以解决简单的任务.

\textit{贪心}(greedy)算法不能保证组合问题的最优解,但是它的计算效率很高.通过贪心算法解决了组合优化问题后,还可以进行fine-tunning.

将贪心算法和预训练结合起来就得到了\textit{贪心有监督预训练}(greedy supervised pretraining)方法.

\textbf{原始的贪心有监督预训练}对神经网络进行逐层预训练.它之所以有效的原因是因为它为中间层提供了一种有效的指导方式.一般来说,预训练可以帮助优化问题收敛和提升泛化能力.此外,迁移学习也与与训练有关联.

\textbf{FitNets方法是另一种预训练方法}.它使用小型的表现良好的网络指导更加复杂的深层神经网络进行训练.不仅预测最终结果,还对中间结果进行预测.

\subsection{Designing Models to Aid Optimization}

本节主要从模型设计的角度去简化优化算法.在实际中,选择函数族比使用优化算法更重要.

当前的神经网络普遍使用线性变换和激活函数进行组合,组合后的函数几乎处处可导并且有较大的斜率.

另外神经元之间的跳跃连接也可以简化优化算法.

\subsection{Continuation Methods and Curriculum Learning}

通常我们希望将初始值选在最优解附近.\textit{continuation methods}是一种策略族,可以确保参数值初始值选取位于目标点附近,以简化优化过程.

传统的continuation methods算法基于平滑目标函数的方法,在保留全局最小值的情况下,尽可能去除局部极小值.这样,一些非凸优化问题可以近似为凸优化问题.尽管在深度学习中,局部极小值点不是优化的主要困难,但是continuation methods仍然是一个帮助提升深度神经网络的主要方法.

\textit{curriculum learning}或者\textit{shaping}方法可以视为一种continuation method.它的基本思路是在学习简单概念的基础上不断迭代,进而学习更加复杂的概念,最终达到所期望的目标概念.在RNN中使用的\textit{stochastic curriculum}将简单的概念和复杂概念进行随机混合用来训练模型,但是在每一轮迭代中,简单概念所占比例逐步降低.与\textit{deterministic curriculum}策略相比,stochastic curriculum的表现要好得多.