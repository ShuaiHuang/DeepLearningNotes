\chapter{Probability and Information Theory}

\setcounter{section}{8}
\section{Common Probability Distributions}

\subsection{Bernoulli Distribution}
\begin{equation}\begin{split}
&P(x=1)=\phi\\
&P(x=0)=1-\phi
\end{split}\end{equation}

\subsection{Multinoulli Distribution}

Bernoulli分布的扩展，假设离散变量有$k$种不同状态。Bernoulli分布和Multinoulli分布在各自的定义域内可以描述任何分布。

\subsection{Gaussian Distribution}

\begin{equation}
\mathcal N(\bm x;\bm{\mu,\beta^{-1}})=\sqrt{\frac{\text{det}(\bm\beta)}{(2\pi)^n}}\exp\Big(-\frac{1}{2}(\bm{x-\mu})^T\bm\beta(\bm{x-\mu})\Big)
\end{equation}
其中$\bm\beta$是精度矩阵，即协方差矩阵的逆矩阵。

\subsection{Exponential and Laplace Distribution}

指数分布：
\begin{equation}
p(x;\lambda)=\lambda\bm{1}_{x\ge 0}\exp(-\lambda x)
\end{equation}
其中$\bm{1}_{x\ge 0}$是指示函数。

拉普拉斯分布
\begin{equation}
\text{Laplace}(x;\mu,\gamma)=\frac{1}{2\gamma}\exp\Big(-\frac{|x-\mu|}{\gamma}\Big)
\end{equation}

\subsection{The Dirac Distribution and Empirical Distribution}

狄利克雷分布：
\begin{equation}
p(x)=\delta(x-\mu)
\end{equation}

经验分布：
\begin{equation}
\hat p(x)=\frac{1}{m}\sum_{i=1}^m\delta(\bm{x-x^{(i)}})
\end{equation}
其中，$\bm x^{(i)}$是从数据集中采样得到的样本集。

\subsection{Mixtures of Distributions}

\begin{equation}
P(x)=\sum_iP(c=i)P(x|c=i)
\end{equation}

\section{Useful Properties of Common Functions}

logistic sigmoid:
\begin{equation}
\sigma(x)=\frac{1}{1+\exp(-x)}
\end{equation}

softplus:
\begin{equation}
\zeta(x)=\log(1+\exp(x))
\end{equation}

\setcounter{section}{12}
\section{Information Theory}

事件$\mathbf x =\bm x$的\textit{自信息}定义为
\begin{equation}
I(\bm x)=-\log P(\bm x)
\end{equation}

我们通常使用\textit{Shannon entropy}量化\textbf{整个概率分布的不确定性}
\begin{equation}
    H(\mathbf x)=\mathbbm E_{\mathbf x\sim P}[I(x)]=\mathbbm E_{\mathbf x\sim P}[\log P(x)]
\end{equation}
也就是说，一个概率分布的Shannon熵也就是基于该分布的事件所包含信息的期望值。

使用\textit{Kullback-Leibler divergence}度量两个分布$P(\mathbf x)$和$Q(\mathbf x)$的差异程度
\begin{equation}
D_{\text{KL}}(P\|Q)=\mathbbm E_{\mathbf x\sim P}\Big[\log\frac{P(x)}{Q(x)}\Big]=
    \mathbbm E_{\mathbf x\sim P}[\log P(x)-\log Q(x)]\end{equation}
需要注意的是$D_{\text{KL}}(P\|Q)\ne D_{\text{KL}}(Q\|P)$.

与\textit{Kullback-Leibler divergence}类似的还有\textit{cross-entropy}
\begin{equation}
    H(P,Q)=H(P)+D_{\text{KL}}(P\|Q)=-\mathbbm E_{\mathbf x\sim P}\log Q(x)
\end{equation}
通常情况下，规定$\lim_{x\to 0}\,x\log x=0$.
