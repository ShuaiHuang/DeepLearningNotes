\chapter{Applications}

\section{Large-Scale Deep Learning}

但个神经元并不能体现出智能的特性,只有大量神经元组合在一起才能体现出智能的特性.

\subsection{Fast CPU Implementations}

精心调试的CPU会有较高的性能,并且有多种策略可供选择,对运行在CPU上的程序进行优化.

\subsection{GPU Implementations}

绝大部分现代神经网络是在GPU上实现的.GPU具有高度并行性和较大内存带宽,但是相比于CPU,它的时钟频率较低,并且分支能力较弱.人工神经网络有同样的特性,因此适合运行在GPU上.基于此,GPU厂商开发出了\textit{通用GPU}(general purpose GPUs)用以处理GPU编程代码,而不仅仅是图形渲染.

但是GPU编程比较困难,大部分工程人员使用现成的代码进行组合以实现自己的模型,尽量避免编写底层代码.

\subsection{Large-Scale Distributed Implementations}

由于单个机器的计算能力有限,因此我们希望可以将训练过程和推断过程转化成分布式计算.

推断过程的分布式计算很简单,因为每个机器可以分别运行模型的不同部分,处理同一个样本,这又被称为\textit{数据并行}(data parallelism).

多个机器在单个数据点上进行协同计算的过程被称为\textit{模型并行}(model parallelism).

\textit{异步SGD}(asynchronous stochastic gradient descent)可以解决数据并行问题.

\subsection{Model Compression}

模型推断时对内存资源的要求更高.通常使用\textit{模型压缩}(model compression)来获取一个对内存需求更少,运行时间更短的小模型来进行存储和执行.

模型压缩可行的前提是原始模型为防止过拟合将不同的模型进行集成,组成了大模型.

\subsection{Dynamic Structure}

对数据处理系统进行加速的一项通用策略是设计带有在图上具有\textit{动态结构}(dynamic structure)的模型去处理输入数据.在神经网络中根据输入使用不同的子网络进行计算的结构被称为\textit{条件计算}(conditional computation).动态结构是从计算机理论到软件工程领域被广泛使用的一项基本原则.

一种动态结构是层级结构分类器,用以提升计算效率.决策树本身就是典型的动态结构.基于同样的思路,\textit{gater}根据当前输入选择不同的专家网络进行计算.当专家网络数目较少时,使用\textit{hard mixture of experts}进行计算.但是当专家网络数目较多时,组合数目也增多,就需要借助额外的手段去训练专家网络的组合了.

另外一种动态结构是开关形式,隐层神经元根据上下文从不同的输入单元获取输入.

需要注意的是,动态结构大大降低了神经网络的并行化操作,往往很难高效地进行编程实现.

\subsection{Specialized Hardware Implementations of Deep Networks}

神经网络有不同的硬件实现方式:
\begin{itemize}
    \item ASICs: application-specific integrated circuit
    \item digital: based on binary representations of numbers
    \item analog: based on physical implementations of continuous values as voltages or currents
    \item hybird implementations: field programmable gated array
\end{itemize}

深度神经网络专用硬件得以发展的原因有两点:第一,可以在推断时候使用较低的精度;第二,通用处理器(CPU/GPU)计算速率增长放缓,需要通过提升并行特性来提升整体性能.

\section{Computer Vision}

\subsection{Preprocessing}

计算机视觉往往需要很少的预处理步骤.最常见的有像素值归一化和输入图像尺寸归一化.对于训练数据集的预处理有\textit{dataset augmentation}.同时对于训练数据集以及测试数据集进行的预处理主要是为了排除数据扰动对模型的影响.通常需要对于输入数据扰动有明确的描述,并且需要确认去除扰动后对模型训练不会产生影响.

\subsubsection{Contrast Normalization}

通常情况下,绝大多数任务中可以去除的样本间差异就是对比度差异.设图像为$X\in\mathcal R^{r\times c\times 3}$,其中$X_{i,j,1}$代表红色通道中第$i$行第$j$列所对应的像素亮度.整幅图像的对比度为
\begin{equation}
\sqrt{\frac{1}{3rc}\sum_{i=1}^{r}\sum_{j=1}^{c}\sum_{k=1}^{3}(X_{i,j,k}-\overline{X})^2}
\end{equation}
其中,$\overline X$是整幅图像的亮度均值
\begin{equation}
\overline X=\frac{1}{3rc}\sum_{i=1}^{r}\sum_{j=1}^{c}\sum_{k=1}^{3}X_{i,j,k}
\end{equation}

\textit{全局对比度归一化}(Global contrast normalization, GCN)旨在防止图像间的对比度有较大差异.常将图像的减去均值亮度,并将亮度分布归一化.即
\begin{equation}
X'_{i,j,k}=s\frac{X_{i,j,k}-\overline X}{\max\Big\{\epsilon,\sqrt{\lambda+\frac{1}{3rc}\sum_{i=1}^{r}\sum_{j=1}^{c}\sum_{k=1}^{3}(X_{i,j,k}-\overline X)^2}\Big\}}
\end{equation}

使用$L^2$范数而非标准差归一化的原因在于,神经网络对于方向的响应要比对于准确位置的响应要好.

需要注意的是,\textit{球化}(sphering)与GCN并不相同.球化是指将主成分的方差进行缩放达到一致.球化通常又被称为\textit{白化}(whitening).

全局对比度归一化并不能对图像中局部细节进行突出显示,由此催生出了\textit{局部对比度归一化}(local contrast normalization, LCN).局部对比度归一化可以通过可分离式卷积进行快速实现.此外,LCN也可作为神经网络的非线性单元.

在实际使用中,需要特别注意LCN的正则化策略.

\subsubsection{Dataset Augmentation}

除了之前介绍的augmentation方法,数据集augumentation有更加高级的变换方式.

\section{Speech Recognition}

语音识别的目的是将包含语言信息的声音信号$\bm X$转换成文字序列$\bm y$.\textit{自动语音识别}(automatic speech recognition)的模型可以概括为
\begin{equation}
f^\ast_{ASR}(\bm X)=\mathop{\arg\max}_{\bm x}P^\ast(\bm y|\mathbf X=\bm X)
\end{equation}

传统方法使用HMM与GMM结合的策略进行语音识别.随着深度模型的发展,许多方法借助于深度神经网络提取特征,并用提取出的特征训练HMM和GMM模型.之后深度模型继续发展,并打破了HMM-GMM模型十多年没有提升的僵局.

语音识别领域内的一项突破是使用卷积神经网络进行语音识别.它以二维的视角看待声音信号.

另外一项突破就是端到端的深度神经网络直接摒弃了HMM模型的使用.其中,声音世界的音素级别的对齐也是一个重要环节.

\section{Natural Language Processing}

\textit{自然语言处理}(natural language processing)是使用计算机对人类语言进行处理的技术.除了神经网络领域内的通用技术外,还需要引入专业领域的知识.

\subsection{$n$-grams}

\textit{语言模型}(language model)是指\textit{标记}(token)序列的概率分布.标记序列定长为$n$的模型被称为$n$-gram.
\begin{equation}
P(x_1,x_2,\cdots,x_\tau)=P(x_1,\cdots,x_{n-1})\prod_{t=n}^\tau P(x_t|x_{t-n+1},\cdots,x_{t-1})
\end{equation}
其中,根据$n$取值不同,$n$-gram可以分为:\textit{unigram}($n=1$),\textit{bigram}($n=2$)和\textit{bigram}($n=3$).

通常使用最大似然度对$n$-gram进行直接训练.并且训练过程中可以对$n-1$-gram一起进行训练.

在句子起始位置的分布通常使用边缘概率分布.

对于条件概率为$0$的情况,需要引入\textit{平滑}(smoothing)策略.一种方法是对所有的概率分布加入非零常量.另一种策略是\textit{back-off}方法,使用低阶$n$-gram模型代替高阶模型.

对于维度灾难问题,使用最近邻查找的方法进行缓解,但不能从根本上克服这一问题.由此引入\textit{基于类别的语言模型}(class-based language model)根据词汇类别共享词汇间的统计信息.但是这种处理方式在信息表示过程中有信息损失.

\subsection{Neural Language Models}

\textit{网络语言模型}(Neural Language Models, NLM)专门用来解决语言模型建模过程中的维度灾难问题.它根据词汇上下文对统计信息进行共享.在相似的句子间建立起关联.

词汇表示又被称为\textit{词汇嵌入}(word embeddings).NLM模型更加注重词汇嵌入这种分布式表示方法.

其他模型也可以使用分布式表示提升模型的性能.

\subsection{High-Dimensional Outputs}

基于词汇的概率分布计算代价很大.最朴素的做法是从隐单元到输入空间建立起放射变换,再用sotmax进行概率估计.

\subsubsection{Use of a Short List}

为减少朴素做法的计算代价,引入有限词汇集,并进一步区分为高频词汇表与低频词汇表.但是泛化能力受到高频词汇的限制.

\subsubsection{Hierarchical Softmax}

另一种减少计算代价的方法是对词汇进行逐级分类,最终形成一棵树.每个单词的概率可以通过到达该词汇所有路径的概率总和得到.

层级结构可以通过机器学习的方法得到.

在训练和测试过程中都会有计算效率高的优势.

同时,这种方法不能解决在给定上下文的时候,相似单词选取的问题.

相比于\textit{基于采样的方法}(sampling-based method),这种方法在实际中的效果并不是很好.

\subsubsection{Importance Sampling}

一种加速NLM训练过程的策略就是避免计算所有词汇出现在下一个位置的概率.但是可以通过对词汇数据集进行采样,计算子集中的每个词汇的概率.
\begin{equation}\begin{split}
\frac{\partial\log P(y|C)}{\partial\bm\theta}&=\frac{\partial\log\text{softmax}_y(\bm a)}{\partial\bm\theta}\\
&=\frac{\partial a_y}{\partial\bm\theta}-\sum_{i}P(y=i|C)\frac{\partial a_i}{\partial\bm\theta}
\end{split}\end{equation}
上式中后一项需要从模型本身进行采样,仍然需要计算所有单词的概率.因此,一般做法是\textit{importance sampling}从另外一个分布中进行采样,并进行修正.针对上述情形,提出了\textit{biased importance sampling},
\begin{equation}
w_i=\frac{p_{n_i}/q_{n_i}}{\sum_{j=1}^Np_{n_j}/q_{n_j}}
\end{equation}
因此有
\begin{equation}
\sum_{i=1}^{|\mathbbm V|}P(i|C)\frac{\partial a_i}{\partial\bm\theta}\approx\frac{1}{m}\sum_{i=1}^mw_i\frac{\partial a_{n_i}}{\partial\bm\theta}
\end{equation}

更一般地,为了加速训练过程,可以使用稀疏输出层.

\subsubsection{Noise-Contrastive Estimation and Ranking Loss}

\textit{Ranking loss}将NLM的输出看做每个单词作为一个分数,并且正确的分数$a_y$排序比其他的分数$a_i$要靠前,对应的损失函数为
\begin{equation}
L=\sum_i\max(0,1-a_y+a_i)
\end{equation}
但是它不能提供条件概率的估计.

\subsection{Combining Neural Language Models with $n$-grams}

$n$-gram模型的一大优势是模型capacity比较高,但是计算量比较低.同时NLM的计算量较高.因此考虑将这两者进行结合.

\subsection{Neural Machine Translation}

机器翻译的任务是读入一种语言的句子,并将其翻译成同等含义的另一种语言的句子.从整体上看,机器翻译系统的一个部分是产生候选翻译选项,另外一个部分是对翻译候选项进行评价.

传统的语言模型仅仅输出自然语言句子的概率,为使其能够满足机器翻译任务的需求,需要将其拓展至条件概率的范畴.

基于MLP模型方法的一大缺陷是语句必须是定长.对于这个问题,可以借助于RNN模型,形成encoder-decoder框架.为了产生基于原始句子的条件概率,这个模型必须能够对原始的句子有一个完整的表示策略.

\subsubsection{Using an Attention Mechanism and Aligning Pieces of Data}

机器翻译的一种高效做法是,一次读入整个句子,然后根据相关的上下文,一次输出一个单词.基于这种思想,提出了\textit{attention}模型,主要包含以下几个主要组成部分
\begin{itemize}
    \item 读入原始数据并转换成分布式表达方法;
    \item 前一步输出的特征向量列表;
    \item 依次对内存中数据进行处理并输出结果的模块.
\end{itemize}

\subsection{Historical Perspective}

略.

\section{Other Applications}

\subsection{Recommender Systems}

推荐系统主要分为两种应用情形:在线广告和商品推荐.推荐系统相关的任务可以归纳为有监督学习:给出用户和商品的相关信息,对用户的兴趣指标进行预测,最终具化为一个回归问题或者分类问题.

常用的一种方法是\textit{协同过滤}(collaborative filtering).可以通过有参数方式和无参数方式分别实现.\textit{双线性预测}(bilinear prediction)实有参数实现方式之一.在具体应用中,双线性预测知识集成学习方法的学习器之一.

除了双线性方法,基于网络实现的协同过滤方法是RBM无向概率模型.

协同过滤方法有一个明显的缺陷就是其冷启动问题.即对于新加入的用户或者商品,算法很容易失效.解决这个问题的方法是引入商品和用户的额外信息.

\subsubsection{Exploration Versus Exploitation}

一方面,推荐系统存在一个问题就是获取到的用户喜好有偏差.对于未给用户进行推荐的商品是无法预测出用户的反应的.另一方面,强化学习可以对任何动作和反馈进行学习,并且强化学习在exploration和exploitation之间取得平衡.决定exploration于exploitation偏好的因素之一是时间尺度.

另外存在一个困难就是如何对比不同的强化学习策略.

\subsection{Knowledge Representation, Reasoning and Question Answering}

\subsubsection{Knowledge, Relations and Question Answering}

在知识表达领域,一个有意思的研究方向就是如何训练出一种表达方式,使得分布式表达能够捕捉到不同实体间的关系.

在数学领域,\textit{二维关系}(binary relation)是描述有序元素对之间关系的集合.

在AI领域,关系指的是由简单句法构成的并且高度结构化语言描述的句子.

为了在神经网络上表示关系并进行推理,首先需要建立训练数据集.通常使用\textit{relational database}和\textit{knowledge base};此外,还需要选择合适的模型族.

知识表达的一种实际应用是\textit{link prediction},但是它的结果很难量化评价.另一种实际应用是\textit{word-sense disambiguation}.