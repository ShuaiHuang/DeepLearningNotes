\chapter{Convolutional Networks}

\textit{卷积神经网络}(convolutional networks)是一种专门用来处理grid-like拓扑关系数据的神经网络.神经网络中至少有一层神经元使用卷积代替矩阵乘法操作.

\section{The Convolution Operation}

最一般形式的卷积的形式为
\begin{equation}
s(t)=\int x(a)w(t-a)da
\end{equation}
其中,$s(t)$被称为\textit{feature map};$x(t)$被称为\textit{input};$w(t)$被称为\textit{kernel}.

在深度神经网络中,常使用离散形式
\begin{equation}
s(t)=(x*w)(t)=\sum_{a=-\infty}^\infty x(a)w(t-a)
\end{equation}
在实际中大多时候是在有限区间上求和.

对于二维空间上的卷积,有
\begin{equation}\begin{split}
S(i,j)=(I*K)(i,j)&=\sum_m\sum_nI(m,n)K(i-m,j-n) \\
&=\sum_m\sum_nI(i-m,j-n)K(m,n)
\end{split}\end{equation}
可见其具有\textit{可交换性}(commutative),这也是为什么进行卷积运算时kernel进行翻转的原因.kernel翻转在数学证明上有很大的便利,但是对于机器学习来说并没有太多的含义.但是在实际中,常根据可交换性,对输入的数据进行翻转而保持kernel不变.

同时需要注意与\textit{cross-correlation}的区别
\begin{equation}
S(i,j)=(I*K)(i,j)=\sum_m\sum_nI(i+m,j+n)K(m,n)
\end{equation}

卷积运算也可以看做是与某个特定矩阵的乘法运算,但是需要注意的是这个特定矩阵是有限制的
\begin{itemize}
    \item 矩阵中的某些项与其他项相等(如\textit{Toeplitz matrix}, \textit{doubly block circulant matrix});
    \item 矩阵是稀疏的,因为与输入图像相比,kernel尺寸是非常小的.
\end{itemize}

\section{Motivation}

在神经网络中引入卷积就引入了\textit{稀疏交互}(sparse interations),\textit{参数共享}(parameter sharing)和\textit{同变性}(equivariant representations).同时,卷积也给神经网络提供了处理可变长输入数据的能力.
\begin{description}
    \item [稀疏交互]
    稀疏交互的原因是因为kernel的尺寸小于输入的尺寸.在神经网络中,稀疏交互相当于\textbf{间接地}与更多的输入进行交互,这样有助于对概念进行抽象.
    \item [参数共享]
    参数共享是指在模型中有多于一个的函数使用相同的参数.在卷积神经网络中,kernel在输入的各个位置上共享.参数共享虽然不能减少计算代价,但是可以降低存储代价.
    \item [同变性]
    参数共享直接导致了对于变换的同变性.如果输入发生变化,那么输出就会以相同的形式进行变化.需要注意的是,卷积并非对于所有的变换都具有同变性,它仍然需要借助于其他的机制去处理不具有同变性的变换.
\end{description}

\section{Pooling}

一层典型的卷积神经网络可以分为三个处理阶段:
\begin{itemize}
    \item 卷积计算
    \item detector stage
    \item pooling function
\end{itemize}
其中pooling用以统计特定区间内的统计量.Pooling有不同的类型\footnote{如何选取不同的类型?各种不同类型pooling的适用范围是什么?}

Pooling对于输入中的微小变换具有近似不变性.本质上来说,Pooling相当于在学习的函数中加入非常强的先验.在实际中,可以在同一个位置pooling不同的卷积结果,这样就可以学习到对于变换具有不变性的特征.

Pooling相当于降维以提高计算效率,同时降低存储开销.

Pooling也是用于处理可变长输入的手段.

Pooling的选取策略有
\begin{itemize}
    \item 不同区域使用不同类型的pooling
    \item 通过学习的方式,在所有区域使用相同类型的pooling
\end{itemize}

Pooling使用top-down信息使得网络结构趋于复杂化.

\section{Convolution and Pooling as Infinitely Strong Prior}

先验可以分为强先验和弱先验两类.弱先验具有较高的熵,强先验具有较低的熵.

卷积神经网络相当于给全连接神经网络加入了极强先验.如果把卷积神经网络当做具有极强先验的全连接神经网络,并在此基础上进行编程实现,将会带来巨大的计算量.但是可以借助于这种观点对卷积神经网络进行数学分析
\begin{itemize}
    \item 卷积神经网络和pooling容易导致欠拟合;
    \item 从学习效果统计量的角度来看,只能用卷积神经网络模型去与其他的卷积神经网络模型作对比.
\end{itemize}

\section{Variants of the Basic Convolution Function}

数学领域内的卷积与神经网络领域内的卷积有一些不同.

神经网络中单层神经网络会使用不同类型的kernel提取出不同类型的特征,神经网络的输入有可能是多维的(有多个channel).多维卷积只有在输入和输出channel相等的情况下才是可交换的(因为channel之间的元素也可能存在线性操作).

可以略过kernel中某些位置计算卷积,以达到降采样的目的.

为了保证输出的尺寸,还需要在输入数据的边缘进行补零操作:
\begin{itemize}
    \item valid convolution 不使用zero-padding;
    \item same convolution 加入zero-padding保证输入和输出的尺寸相同;
    \item full convolution 加入zero-padding保证输入数据的每个位置都被访问$k$次($k$是kernel中的元素个数).
\end{itemize}
对于full convolution,不能只用一种类型的kernel学习特征.最优的卷积通常介于valid convolution与same convolution之间.

\textit{局部连接层}(locally connected layer)在结构上(邻接矩阵adjacency matrix)上与卷及神经网络结构相同,但是它没有参数共享,有时它也被称为\textit{unshared convolution}.也可以对卷积层或者局部连接层加上严格的限制,以降低存储以及统计的消耗.\textit{tiled convolution}相当于在在这两者之间做了一个折中,卷积的类型在空间上有周期性的规律.
\begin{equation}
Z_{i,j,k}=\sum_{l,m,n}V_{i,j+m-1,k+n-1}K_{i,l,m,n,j\%t+1,k\%t+1}
\end{equation}
如果detector单元有不同类型的输入,并且max-pooled学习到了不同类型的输入并且学习到了不变性特征,那么整个卷积层也就有相应的不变性性质.

在前文中提到了,可以使用输入与单个矩阵相乘的观点看待卷积操作.在误差逆传播过程中,相当于乘以上述单个矩阵的转置的操作\footnote{TODO: 补充一下}.卷积,从输出到权重的逆传播,从输入到输入的逆传播这三种操作满足任何类型的前向卷积神经网络的训练过程.

通常还会在非线性变换之前加入bias,bias共享参数的策略有
\begin{itemize}
    \item 局部连接矩阵不共享参数;
    \item tiled卷积神经网络相同的tiling pattern共享参数;
    \item 卷及神经网络共享相同的参数;
    \item 如果输入是已知的并且尺寸大小固定,则可以通过学习得到不同的参数.虽然这样降低了统计效率,但是可以修正图像在不同位置的统计偏差.
\end{itemize}

\section{Structured Outputs}

卷积神经网络可以输出结构化的目标数据(张量).但是其中会存在一个问题,输出平面的尺寸会比输入平面的尺寸小.有三种方法可以解决这一问题
\begin{itemize}
    \item 避免pooling操作;
    \item 输出低分辨率标签网格;
    \item 在单位stride上执行pooling操作.
\end{itemize}
此外还有一种方法就是通过迭代的方法输出pixel-wise标记图像.

需要注意,结构化输出的前提是,大量连续的像素族总是倾向于有相同的标签.

\section{Data Types}

卷积神经网络的一大优势是它们也可以处理输入在空间范围内变化的数据.卷及神经网络会根据输入图像的尺寸做卷积,并对卷积结果(feature map)所尺寸缩放.需要注意,只有可变尺寸的输入数据中包含有同样的pattern时,上述处理方式才会生效.

\section{Efficient Convolution Algorithms}

一般可以通过选择合适的卷计算法来加速训练过程.

卷积等价于将信号和卷积核使用傅里叶变换转换到频域中做点对点相乘,然后再经过傅里叶逆变换转换回原始空时域.

一种方法是利用\textit{可分离核}(seperable kernel)减少计算量,但是这种方法也存在局限性.因为不是每次都会使用可分离核.

因此,精确计算或者估计卷积是一个热门的研究方向.快速计算卷积不仅有利于训练过程,对于提升预测过程的效率也有和大帮助.部署一个卷积神经网络所需要的资源也是值得关注的.

\section{Random or Unsupervised Feature}

特征学习环节的计算代价较高.一个解决方案就是不使用有监督学习方法去进行训练.在卷积神经网络中,kernel生成方法有
\begin{itemize}
    \item 随机初始化
    \item 手工设计
    \item 无监督学习方法
\end{itemize}
无监督学习得到的特征与最终的分类层相互独立.可以通过改变最终的分类层,将模型用于不同的任务.

随机filter在卷积神经网络上效果很好,可以借助这个方法进行模型选择,以较低的计算开销计算模型选择.

另一种间接的特征学习方法是不适用全部的前向与逆向传播(类似于前向神经网络中的贪心逐层初始化).进一步地,在卷积神经网络中使用局部初始化进行特征学习.但是这种方法使用较少.

\section{The Neuroscientific Basis for Convolutional Networks}

神经网络本质的设计规则是从神经科学中得到的.其中卷积神经网络与人脑V1区域相对应,卷积神经网络符合V1的三种特性
\begin{enumerate}
    \item V1神经元按照空间映射关系进行排列;
    \item V1中包含\textit{简单神经元}(simple cells),与卷积神经网络中的detector units对应;
    \item V1中包含\textit{复杂神经元}(complex cells),与卷积神经网络中的pooling units以及cross-channel pooling strategies相对应.
\end{enumerate}

高层的脑神经对概念有反馈,并且对于各种几何变换具有不变性.与卷积神经网络中的最后一层特征曾比较相似的脑区域是inferotemporal cortex(IT).

同时卷积神经网络与人类视觉系统有以下区别
\begin{enumerate}
    \item 人眼除了被称为\textit{fovea}的patch外的分辨率很低;
    \item 人类视觉系统常常与其他感官系统结合;
    \item 人类视觉系统的任务不仅仅只是物体识别;
    \item 即使是简单的V1区域仍然深深地受到高层神经元的影响.在神经网络模型中,反馈被广泛应用,但是并没有有实质性的发展;
    \item 前向IT捕捉到的信息与卷积神经网络捕捉到的特征很相似,但是它们的中间过程的相似程度仍然未知.
\end{enumerate}

脑神经科学并没有给出如何训练卷积神经网络的方法.

通常通过\textit{reverse correlation}理解单个神经元对应的函数.对于大部分V1神经元细胞,reverse correlation展示出它们与Gabor函数很类似.
\begin{equation}\label{eq:gabor_function}
w(x,y;\alpha,\beta_x,\beta_y,f,\phi,x_0,y_0,\tau)=\alpha\exp(-\beta_xx'^2-\beta_yy'^2)\cos(fx'+y)
\end{equation}
其中
\begin{equation}\begin{split}
x'&=(x-x_0)\cos(\tau)+(y-y_0)\sin(\tau)\\
y'&=-(x-x_0)\sin(\tau)+(y-y_0)\cos(\tau)
\end{split}\end{equation}
$\alpha,\beta_x,\beta_y,f,\phi,x_0,y_0,\tau$是Gabor函数的控制参数.式\ref{eq:gabor_function}中Gaussian乘子可以被视为门禁因子;余弦乘子$\cos(fx'+\phi)$用来控制简单细胞对$x'$方向上的亮度变化的反应.

值得注意的是,通过机器学习得到的特征与V1得到的特征很接近.

\section{Convolutional Networks and the History of Deep Learning}

卷积神经网络在深度神经发展历史中具有重要意义
\begin{itemize}
    \item 卷积神经网络具有坚实的理论基础;
    \item 卷积神经网络是首先被训练出的深度学习模型之一;
    \item 卷积神经网络具有广阔的商业应用前景.
\end{itemize}

卷积神经网络在各种图像相关竞赛中表现出众.

卷积神经网络是BP算法首先被成功应用的模型之一.

总体来看,卷积神经网络被广泛用来处理具有清晰网络拓扑结构的数据(一般是二维数据).下一章介绍的RNN专门用来处理一维数据.