\chapter{Convolutional Networks}

\textit{卷积神经网络}(convolutional networks)是一种专门用来处理grid-like拓扑关系数据的神经网络.神经网络中至少有一层神经元使用卷积代替矩阵乘法操作.

\section{The Convolution Operation}

最一般形式的卷积的形式为
\begin{equation}
s(t)=\int x(a)w(t-a)da
\end{equation}
其中,$s(t)$被称为\textit{feature map};$x(t)$被称为\textit{input};$w(t)$被称为\textit{kernel}.

在深度神经网络中,常使用离散形式
\begin{equation}
s(t)=(x*w)(t)=\sum_{a=-\infty}^\infty x(a)w(t-a)
\end{equation}
在实际中大多时候是在有限区间上求和.

对于二维空间上的卷积,有
\begin{equation}\begin{split}
S(i,j)=(I*K)(i,j)&=\sum_m\sum_nI(m,n)K(i-m,j-n) \\
&=\sum_m\sum_nI(i-m,j-n)K(m,n)
\end{split}\end{equation}
可见其具有\textit{可交换性}(commutative),这也是为什么进行卷积运算时kernel进行翻转的原因.kernel翻转在数学证明上有很大的便利,但是对于机器学习来说并没有太多的含义.但是在实际中,常根据可交换性,对输入的数据进行翻转而保持kernel不变.

同时需要注意与\textit{cross-correlation}的区别
\begin{equation}
S(i,j)=(I*K)(i,j)=\sum_m\sum_nI(i+m,j+n)K(m,n)
\end{equation}

卷积运算也可以看做是与某个特定矩阵的乘法运算,但是需要注意的是这个特定矩阵是有限制的
\begin{itemize}
    \item 矩阵中的某些项与其他项相等(如\textit{Toeplitz matrix}, \textit{doubly block circulant matrix});
    \item 矩阵是稀疏的,因为与输入图像相比,kernel尺寸是非常小的.
\end{itemize}

\section{Motivation}

在神经网络中引入卷积就引入了\textit{稀疏交互}(sparse interations),\textit{参数共享}(parameter sharing)和\textit{同变性}(equivariant representations).同时,卷积也给神经网络提供了处理可变长输入数据的能力.
\begin{description}
    \item [稀疏交互]
    稀疏交互的原因是因为kernel的尺寸小于输入的尺寸.在神经网络中,稀疏交互相当于\textbf{间接地}与更多的输入进行交互,这样有助于对概念进行抽象.
    \item [参数共享]
    参数共享是指在模型中有多于一个的函数使用相同的参数.在卷积神经网络中,kernel在输入的各个位置上共享.参数共享虽然不能减少计算代价,但是可以降低存储代价.
    \item [同变性]
    参数共享直接导致了对于变换的同变性.如果输入发生变化,那么输出就会以相同的形式进行变化.需要注意的是,卷积并非对于所有的变换都具有同变性,它仍然需要借助于其他的机制去处理不具有同变性的变换.
\end{description}

\section{Pooling}

一层典型的卷积神经网络可以分为三个处理阶段:
\begin{itemize}
    \item 卷积计算
    \item detector stage
    \item pooling function
\end{itemize}
其中pooling用以统计特定区间内的统计量.Pooling有不同的类型\footnote{如何选取不同的类型?各种不同类型pooling的适用范围是什么?}

Pooling对于输入中的微小变换具有近似不变性.本质上来说,Pooling相当于在学习的函数中加入非常强的先验.在实际中,可以在同一个位置pooling不同的卷积结果,这样就可以学习到对于变换具有不变性的特征.

Pooling相当于降维以提高计算效率,同时降低存储开销.

Pooling也是用于处理可变长输入的手段.

Pooling的选取策略有
\begin{itemize}
    \item 不同区域使用不同类型的pooling
    \item 通过学习的方式,在所有区域使用相同类型的pooling
\end{itemize}

Pooling使用top-down信息使得网络结构趋于复杂化.

\section{Convolution and Pooling as Infinitely Strong Prior}

先验可以分为强先验和弱先验两类.弱先验具有较高的熵,强先验具有较低的熵.

卷积神经网络相当于给全连接神经网络加入了极强先验.如果把卷积神经网络当做具有极强先验的全连接神经网络,并在此基础上进行编程实现,将会带来巨大的计算量.但是可以借助于这种观点对卷积神经网络进行数学分析
\begin{itemize}
    \item 卷积神经网络和pooling容易导致欠拟合;
    \item 从学习效果统计量的角度来看,只能用卷积神经网络模型去与其他的卷积神经网络模型作对比.
\end{itemize}